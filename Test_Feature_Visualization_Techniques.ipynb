{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/utkuozbulak/pytorch-cnn-visualizations\n",
    "https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "from scipy.io import loadmat\n",
    "import cv2\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import csv\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1060 6GB'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet=torchvision.models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\binxu/.cache\\torch\\checkpoints\\vgg16-397923af.pth\n",
      "94.5%"
     ]
    }
   ],
   "source": [
    "net = torchvision.models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])  # Note without normalization, the\n",
    "denormalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    def __init__(self, module):\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features = output#torch.tensor(output,requires_grad=True).cuda()\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "def val_tfms(img_np):\n",
    "    img = torch.from_numpy(img_np.astype(np.float32)).permute(2, 0, 1)\n",
    "    nimg = normalize(img).unsqueeze(0).cuda()\n",
    "    return nimg\n",
    "\n",
    "def val_detfms(img_tsr):\n",
    "    img = denormalize(img_tsr.squeeze()).permute(1,2,0)\n",
    "    return img.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TRIAL = 100\n",
    "class FilterVisualizer():\n",
    "    def __init__(self, model, size=56, upscaling_steps=12, upscaling_factor=1.2):\n",
    "        self.size, self.upscaling_steps, self.upscaling_factor = size, upscaling_steps, upscaling_factor\n",
    "        self.model = model #alexnet.features.cuda().eval()\n",
    "        # set_trainable(self.model, False)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def visualize(self, layer, filter, lr=0.05, opt_steps=20, blur=None):\n",
    "        sz = self.size\n",
    "        img = np.uint8(np.random.uniform(50, 250, (sz, sz, 3))) / 255  # generate random image\n",
    "        activations = SaveFeatures(list(self.model.children())[layer])  # register hook\n",
    "\n",
    "        for _ in range(self.upscaling_steps):  # scale the image up upscaling_steps times\n",
    "            # train_tfms, val_tfms = tfms_from_model(vgg16, sz)\n",
    "            img_var = Variable(val_tfms(img), requires_grad=True) # convert image to Variable that requires grad\n",
    "            optimizer = torch.optim.Adam([img_var], lr=lr, weight_decay=1e-6)\n",
    "            for n in range(MAX_TRIAL):\n",
    "                optimizer.zero_grad()\n",
    "                self.model(img_var)\n",
    "                loss = -activations.features[0, filter].mean()\n",
    "                loss.backward()\n",
    "                if img_var.grad.norm()<1E-6:\n",
    "                    img_var = Variable(val_tfms(img + np.random.randn(*img.shape)), requires_grad=True) # convert image to Variable that requires grad\n",
    "                    optimizer = torch.optim.Adam([img_var], lr=lr, weight_decay=1e-6)\n",
    "                    print(\"Optimizer restart\")\n",
    "                else:\n",
    "                    break\n",
    "            for n in range(opt_steps):  # optimize pixel values for opt_steps times\n",
    "                optimizer.zero_grad()\n",
    "                self.model(img_var)\n",
    "                loss = -activations.features[0, filter].mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                print(loss.data.cpu())\n",
    "            img = val_detfms(img_var.data.cpu())\n",
    "            self.output = img\n",
    "            plt.figure(figsize=[8,8])\n",
    "            plt.imshow(FVis.output)\n",
    "            plt.show()\n",
    "            sz = int(self.upscaling_factor * sz)  # calculate new image size\n",
    "            img = cv2.resize(img, (sz, sz), interpolation=cv2.INTER_CUBIC)  # scale image up\n",
    "            if blur is not None: img = cv2.blur(img, (blur, blur))  # blur image to reduce high frequency patterns\n",
    "        self.save(layer, filter)\n",
    "        activations.close()\n",
    "\n",
    "    def save(self, layer, filter):\n",
    "        plt.imsave(\"layer_\" + str(layer) + \"_filter_\" + str(filter) + \".jpg\", np.clip(self.output, 0, 1))\n",
    "\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: all CUDA-capable devices are busy or unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-02b0ed44523b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malexnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ponce\\.conda\\envs\\caffe36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \"\"\"\n\u001b[1;32m--> 305\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ponce\\.conda\\envs\\caffe36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ponce\\.conda\\envs\\caffe36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ponce\\.conda\\envs\\caffe36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \"\"\"\n\u001b[1;32m--> 305\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: all CUDA-capable devices are busy or unavailable"
     ]
    }
   ],
   "source": [
    "feat = alexnet.features.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = net.features.cuda().eval()\n",
    "FVis = FilterVisualizer(feat, size=227, upscaling_steps=2, upscaling_factor=1.2)\n",
    "FVis.visualize(14, 20, blur=10, opt_steps=20)\n",
    "plt.figure(figsize=[8,8])\n",
    "plt.imshow(FVis.output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = alexnet.features[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 224\n",
    "lr = 0.1\n",
    "opt_steps = 100\n",
    "upscaling_factor = 1.2\n",
    "filter = 10\n",
    "loss_arr = []\n",
    "activations = SaveFeatures(list(alexnet.features.children())[8])  # register hook\n",
    "\n",
    "img = np.uint8(np.random.uniform(150, 180, (sz, sz, 3))) / 255\n",
    "img_var = Variable(val_tfms(img), requires_grad=True) # convert image to Variable that requires grad\n",
    "optimizer = torch.optim.Adam([img_var], lr=lr, weight_decay=1e-6)\n",
    "for n in range(opt_steps):  # optimize pixel values for opt_steps times\n",
    "    optimizer.zero_grad()\n",
    "    alexnet.features(img_var)\n",
    "    loss = -activations.features[0, filter].mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_arr.append(loss.data.cpu())\n",
    "img = val_detfms(img_var.data.cpu()).numpy()\n",
    "output = img\n",
    "sz = int(upscaling_factor * sz)  # calculate new image size\n",
    "img = cv2.resize(img, (sz, sz), interpolation=cv2.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet.features(img_var)\n",
    "loss = -activations.features[0, filter].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FVis.output[:,:,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_np = np.random.rand(5,5,3)\n",
    "img_tsr = val_tfms(img_np)\n",
    "img_out = val_detfms(img_tsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing Version of FilterVisualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def np2tensor(image,dtype):\n",
    "#     \"Convert np.array (sz,sz,3) to tensor (1,3,sz,sz), imagenet normalized\"\n",
    "\n",
    "#     a = np.asarray(image)\n",
    "#     if a.ndim==2 : a = np.expand_dims(a,2)\n",
    "#     a = np.transpose(a, (1, 0, 2))\n",
    "#     a = np.transpose(a, (2, 1, 0))\n",
    "    \n",
    "#     #Imagenet norm\n",
    "#     mean=np.array([0.485, 0.456, 0.406])[...,np.newaxis,np.newaxis]\n",
    "#     std = np.array([0.229, 0.224, 0.225])[...,np.newaxis,np.newaxis]\n",
    "#     a = (a-mean)/std\n",
    "#     a = np.expand_dims(a,0)\n",
    "#     return torch.from_numpy(a.astype(dtype, copy=False) )\n",
    "\n",
    "# def tensor2np(img_tensor):\n",
    "#     \"Convert tensor (1,3,sz,sz) back to np.array (sz,sz,3), imagenet DEnormalized\"\n",
    "#     a = np.squeeze(to_np(img_tensor))\n",
    "    \n",
    "#     mean=np.array([0.485, 0.456, 0.406])[...,np.newaxis,np.newaxis]\n",
    "#     std = np.array([0.229, 0.224, 0.225])[...,np.newaxis,np.newaxis]\n",
    "#     a = a*std + mean\n",
    "#     return np.transpose(a, (1,2,0))\n",
    "\n",
    "class FilterVisualizer():\n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "        self.weights = None\n",
    "\n",
    "    def visualize(self, sz, layer, filter, weights=None, \n",
    "                  upscaling_steps=12, upscaling_factor=1.2, lr=0.1, opt_steps=20, blur=None, print_losses=False):\n",
    "        '''Add weights to support visualize combination of channels'''\n",
    "        if weights is not None:\n",
    "            assert len(weights) == len(filter)\n",
    "            self.weights = torch.tensor(weights,dtype=torch.float,device='cuda')\n",
    "        img = (np.random.random((sz,sz, 3)) * 20 + 128.)/255 # value b/t 0 and 1        \n",
    "        activations = SaveFeatures(layer)  # register hook\n",
    "\n",
    "        for i in range(upscaling_steps):  \n",
    "            # convert np to tensor + channel first + new axis, and apply imagenet norm\n",
    "            img_tensor = val_tfms(img)#,np.float32)\n",
    "            img_tensor = img_tensor.cuda()\n",
    "            img_tensor.requires_grad_();\n",
    "            if not img_tensor.grad is None:\n",
    "                img_tensor.grad.zero_(); \n",
    "            \n",
    "            \n",
    "            optimizer = torch.optim.Adam([img_tensor], lr=0.1, weight_decay=1e-6)\n",
    "            if i > upscaling_steps/2:\n",
    "                opt_steps_ = int(opt_steps*1.3)\n",
    "            else:\n",
    "                opt_steps_ = opt_steps\n",
    "            for n in range(opt_steps_):  # optimize pixel values for opt_steps times\n",
    "                optimizer.zero_grad()\n",
    "                _=self.model(img_tensor)\n",
    "                if weights is None:\n",
    "                    loss = -1*activations.features[0, filter].mean()\n",
    "                else: \n",
    "                    loss = -1*torch.einsum(\"ijk,i->jk\", activations.features[0, filter], self.weights).mean()\n",
    "                if print_losses:\n",
    "                    if i%3==0 and n%5==0:\n",
    "                        print(f'{i} - {n} - {float(-loss)}')\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # convert tensor back to np\n",
    "            img = val_detfms(img_tensor)\n",
    "            self.output = img\n",
    "            sz = int(upscaling_factor * sz)  # calculate new image size\n",
    "#             print(f'Upscale img to: {sz}')\n",
    "            img = cv2.resize(img, (sz, sz), interpolation = cv2.INTER_CUBIC)  # scale image up\n",
    "            if blur is not None: img = cv2.blur(img,(blur,blur))  # blur image to reduce high frequency patterns\n",
    "                \n",
    "        activations.close()\n",
    "        return np.clip(self.output, 0, 1)\n",
    "    \n",
    "    def get_transformed_img(self,img,sz):\n",
    "        '''\n",
    "        Scale up/down img to sz. Channel last (same as input)\n",
    "        image: np.array [sz,sz,3], already divided by 255\"    \n",
    "        '''\n",
    "        return cv2.resize(img, (sz, sz), interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    def most_activated(self, img, layer):\n",
    "        '''\n",
    "        image: np.array [sz,sz,3], already divided by 255\"    \n",
    "        '''\n",
    "        img = cv2.resize(img, (224,224), interpolation = cv2.INTER_CUBIC)\n",
    "        activations = SaveFeatures(layer)\n",
    "        img_tensor = val_tfms(img)#,np.float32)\n",
    "        img_tensor = img_tensor.cuda()\n",
    "        \n",
    "        _=self.model(img_tensor)\n",
    "        mean_act = [np.squeeze(to_np(activations.features[0,i].mean())) for i in range(activations.features.shape[1])]\n",
    "        activations.close()\n",
    "        return mean_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = alexnet.features.cuda().eval()\n",
    "FVis = FilterVisualizer(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = FVis.visualize(sz=227, layer=feat[8], filter=[1,5,3,10], weights=[1,3,1,7], blur=10, opt_steps=20, upscaling_steps=3, upscaling_factor=1.2, print_losses=True)\n",
    "plt.figure(figsize=[8,8])\n",
    "plt.imshow(FVis.output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GAN to visualize Convolutiona Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGR_mean = torch.tensor([104.0, 117.0, 123.0])\n",
    "BGR_mean = torch.reshape(BGR_mean, (1, 3, 1, 1))\n",
    "def visualize(G, code):\n",
    "    \"\"\"Do the De-caffe transform (Validated)\"\"\"\n",
    "    code = code.reshape(-1, 4096).astype(np.float32)\n",
    "    blobs = G(torch.from_numpy(code))\n",
    "    out_img = blobs['deconv0']  # get raw output image from GAN\n",
    "    clamp_out_img = torch.clamp(out_img + BGR_mean, 0, 255)\n",
    "    vis_img = clamp_out_img[:, [2, 1, 0], :, :].permute([2, 3, 1, 0]).squeeze() / 255\n",
    "    return vis_img\n",
    "\n",
    "def visualize_for_torchnet(G, code):\n",
    "    \"\"\"Do the De-caffe transform (Validated)\"\"\"\n",
    "    blobs = G(code)\n",
    "    out_img = blobs['deconv0']  # get raw output image from GAN\n",
    "    clamp_out_img = torch.clamp(out_img + BGR_mean, 0, 255) / 255\n",
    "    vis_img = clamp_out_img[:, [2, 1, 0], :, :] # still use BCHW sequence \n",
    "    return vis_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterVisualizerGAN():\n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "        self.G = load_generator()\n",
    "        self.weights = None\n",
    "\n",
    "    def visualize(self, sz, layer, filter, weights=None, \n",
    "                  lr=0.1, opt_steps=20, blur=None, print_losses=False): #upscaling_steps=12, upscaling_factor=1.2, \n",
    "        '''Add weights to support visualize combination of channels'''\n",
    "        if weights is not None:\n",
    "            assert len(weights) == len(filter)\n",
    "            self.weights = torch.tensor(weights,dtype=torch.float,device='cuda')\n",
    "        \n",
    "        activations = SaveFeatures(layer)  # register hook\n",
    "        feat = 0.01 * np.random.rand(1, 4096)\n",
    "        feat = torch.from_numpy(np.float32(feat))\n",
    "        feat = Variable(feat, requires_grad = True).cuda()\n",
    "        img = visualize_for_torchnet(self.G, feat)   \n",
    "        resz_img = F.interpolate(img, (sz, sz), mode='bilinear', align_corners=True)\n",
    "        #  img = (np.random.random((sz,sz, 3)) * 20 + 128.)/255 # value b/t 0 and 1     \n",
    "        # img_tensor = val_tfms(resz_img) \n",
    "        img_tensor = normalize(resz_img.squeeze()).unsqueeze(0)\n",
    "#         img_tensor = img_tensor.cuda()\n",
    "#         img_tensor.requires_grad_();\n",
    "        optimizer = optim.SGD([feat], lr=0.05,momentum=0.3,dampening=0.1)\n",
    "        if not img_tensor.grad is None:\n",
    "            img_tensor.grad.zero_(); \n",
    "\n",
    "        for n in range(opt_steps_):  # optimize pixel values for opt_steps times\n",
    "            optimizer.zero_grad()\n",
    "            img = visualize_for_torchnet(self.G, feat)   \n",
    "            resz_img = F.interpolate(img, (sz, sz), mode='bilinear', align_corners=True)\n",
    "            img_tensor = normalize(resz_img.squeeze()).unsqueeze(0)\n",
    "            _=self.model(img_tensor)\n",
    "            if weights is None:\n",
    "                loss = -1*activations.features[0, filter].mean()\n",
    "            else: \n",
    "                loss = -1*torch.einsum(\"ijk,i->jk\", activations.features[0, filter], self.weights).mean()\n",
    "            if print_losses:\n",
    "                if n%5==0:\n",
    "                    print(f'{n} - {float(-loss)}')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # convert tensor back to np\n",
    "        img = val_detfms(img_tensor)\n",
    "        self.output = img\n",
    "        # sz = int(upscaling_factor * sz)  # calculate new image size\n",
    "#             print(f'Upscale img to: {sz}')\n",
    "        img = cv2.resize(img, (sz, sz), interpolation = cv2.INTER_CUBIC)  # scale image up\n",
    "        if blur is not None: img = cv2.blur(img,(blur,blur))  # blur image to reduce high frequency patterns\n",
    "                \n",
    "        activations.close()\n",
    "        return np.clip(self.output, 0, 1)\n",
    "    \n",
    "    def get_transformed_img(self,img,sz):\n",
    "        '''\n",
    "        Scale up/down img to sz. Channel last (same as input)\n",
    "        image: np.array [sz,sz,3], already divided by 255\"    \n",
    "        '''\n",
    "        return cv2.resize(img, (sz, sz), interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "#     def most_activated(self, img, layer):\n",
    "#         '''\n",
    "#         image: np.array [sz,sz,3], already divided by 255\"    \n",
    "#         '''\n",
    "#         img = cv2.resize(img, (224,224), interpolation = cv2.INTER_CUBIC)\n",
    "#         activations = SaveFeatures(layer)\n",
    "#         img_tensor = val_tfms(img)#,np.float32)\n",
    "#         img_tensor = img_tensor.cuda()\n",
    "        \n",
    "#         _=self.model(img_tensor)\n",
    "#         mean_act = [np.squeeze(to_np(activations.features[0,i].mean())) for i in range(activations.features.shape[1])]\n",
    "#         activations.close()\n",
    "#         return mean_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_net_utils import visualize, load_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = alexnet.features.cuda().eval()\n",
    "FVisG = FilterVisualizerGAN(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = FVisG.visualize(sz=227, layer=feat[8], filter=[1,5,3,10], weights=[1,3,1,7], blur=10, opt_steps=20, upscaling_steps=3, upscaling_factor=1.2, print_losses=True)\n",
    "plt.figure(figsize=[8,8])\n",
    "plt.imshow(FVis.output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-caffe36]",
   "language": "python",
   "name": "conda-env-.conda-caffe36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
